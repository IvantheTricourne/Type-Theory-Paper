\title{Dependent Types and Martin-L\"of Type Theory: An Introduction}
%% \title{Dependent Types and Type Theory: An Introduction}
\author{Carl Factora}
\date{\today}

\documentclass[12pt]{article}
% The following packages are needed because unicode
% is translated (using the next set of packages) to
% latex commands. You may need more packages if you
% use more unicode characters:
\usepackage{amssymb}
\usepackage{bbm}
\usepackage[greek,english]{babel}
% This handles the translation of unicode to latex:
\usepackage{ucs}
\usepackage[utf8x]{inputenc}
\usepackage{autofe}
\usepackage{tikz}
\usetikzlibrary{matrix}
\usetikzlibrary{arrows}

% Some characters that are not automatically defined
% (you figure out by the latex compilation errors you get),
% and you need to define:
\DeclareUnicodeCharacter{8988}{\ensuremath{\ulcorner}}
\DeclareUnicodeCharacter{8989}{\ensuremath{\urcorner}}
\DeclareUnicodeCharacter{8803}{\ensuremath{\overline{\equiv}}}
\DeclareUnicodeCharacter{120738}{\ensuremath{\Sigma}}
\DeclareUnicodeCharacter{955}{\ensuremath{\lambda}}
\DeclareUnicodeCharacter{2261}{\ensuremath{\equiv}}

% Add more as you need them (shouldn‚Äôt happen often).
% Using ‚Äú\newenvironment‚Äù to redefine verbatim to
% be called ‚Äúcode‚Äù doesn‚Äôt always work properly. 
% You can more reliably use:
\usepackage{fancyvrb}

\DefineVerbatimEnvironment
  {code}{Verbatim}
  {} % Add fancy options here if you like.

\begin{document}
\maketitle

%% \begin{abstract}
%% \end{abstract}

\section{Introduction}
We present an introduction to dependent types and (Martin-L\"of) type theory.
At its core, type theory seeks to serve as a possible foundation of mathematics,
one that is based on a different logical foundation. This proof-relevant
form of mathematics is the basis in which types can be used to model logical
propositions and is the core idea of type theory. As such, not only does type
theory have a viable application in mathematics but also a growing interest
in the field of computer science where types find themselves in great use in
many popular programming languages. In both fields, type theory provides a
richer world of types and expressibiilty.

The study of type theory goes hand in hand with the study of dependent types,
which is why we first take the time to familiarize ourselves with them
(Section 2). We then continue with the fundamental concepts of type theory of
Per Martin-L\"of \cite{}. For this, we recommend the reader have a running proof
assistant, such as Agda. In our examples, we refer to Agda syntax rules, and
using a proof assistant should help develop the necessary thought processes to
successfully do type theoretic proofs. 

\section{The World of Dependent Types}
In this section, we introduce the concept of dependent types and their role in
type theoretic systems. At their core, dependent types can be thought of simply
as an \textit{interplay of types and terms}. To understand this relationship,
it helps to think of the ways in which types and terms can \textit{depend} on
one another. Altogether, this results in four dependencies:

\begin{enumerate}
\item terms depending on terms
\item terms depending on types
\item types depending on types
\item types depending on terms
\end{enumerate}
These dependecies, however, should not be confused with dependent types.
In fact, only (2) and (4) correspond to expressions that are typed (necessarily)
with a dependent type (we explain later why (1) and (3) are not typed with
dependent types).

What follows is a brief description of each form of dependency and the
corresponding \(\lambda\)-calculus that is associated with it. With this, we
introduce the (fully-dependent) system of the \textit{Calculus of Constructions}
(\(\lambda C\)) \cite{}. Here, the word \textit{fully} signifies that \(\lambda C\)
includes all four forms of dependency mentioned above.

\subsection*{Simply-Typed (STLC)}
\subsubsection*{Role: Base System}
The STLC is not a dependently typed system. In fact, the term {\em dependent type}
is drawn in constrast with the term {\em simple type}, where the STLC derives
its namesake. A simple type ({\tt t}) is either a {\em primitive type}, {\tt Prim},
which is the set of all types ``pre-defined'' (e.g. lists, numbers, booleans),
or a {\em function type} between other simple types. Thus, simple types have the
following grammar:
\begin{center}
\begin{minipage}{0.5\textwidth}
\begin{code}
Prim || t ‚Üí t
\end{code}
\end{minipage}
\end{center}
The terms of the STLC are the terms of the \textit{untyped \(\lambda\)-calculus},
with the exception of terms that use \textit{self-application} (i.e., Y
combinator, Omega, etc.). An example term, annotated with its type, is generally
written as follows:
\begin{center}
\begin{minipage}{0.5\textwidth}
\begin{code}
example : ‚Ñï ‚Üí ‚Ñï
example = Œª (n : ‚Ñï) ‚Üí n
\end{code}
\end{minipage}
\end{center}
Or, equivalently,
\begin{center}
\begin{minipage}{0.5\textwidth}
\begin{code}
example = Œª n ‚Üí n
\end{code}
\end{minipage}
\end{center}
whenever the type of the argument (viz. {\tt n}) is easily inferable. In this
case, since {\tt example} is of type {\tt ‚Ñï ‚Üí ‚Ñï}, the argument {\tt n} has
to be of type {\tt ‚Ñï}, therefore leaving its type annotation unnecessary.

The STLC has limited expressibilty but has the benefit of no longer having any
infinite calculations \cite{} and is thus the appropriate foundation for
deriving a type theoretic system. This is because the STLC lends itself quite
seamlessly as component of larger, more expressive systems. This flexibility is
what we exploit to derive \(\lambda C\). To do this, we first extend the STLC
with more expressive types through simple extentions of its grammar of simple
types.

The key difference between the STLC and \(\lambda C\) are the places in which
terms and types are allowed to be. In the STLC, terms and types are entirely
seperate entities in that terms only exist on {\em term level} and types
on {\em type level}. We return to the idea of {\em levels} later. Thus, as we
mentioned earlier, this puts a constraint on the system to only allow
\(\lambda\)-abstraction and function application to happen at term level
(viz. definition of simple type). For example, the \textit{constant function}
over types \(\mathbb{N}\) and \(\mathbb{B}\) in the STLC is defined:
\begin{center}
\begin{minipage}{0.5\textwidth}
\begin{code}
const : ‚Ñï ‚Üí ùîπ ‚Üí ‚Ñï
const = Œª n b ‚Üí n
\end{code}
\end{minipage}
\end{center}
Given that {\tt const} is of type {\tt ‚Ñï ‚Üí ùîπ ‚Üí ‚Ñï}, the term that inhabits this
type is {\tt Œª n b ‚Üí n}, where {\tt n} and {\tt b} are typed {\tt ‚Ñï} and {\tt ùîπ},
respectively. Here, we are also limited by the fact that this constant function can
only be applied to natural numbers and booleans. It is not the most general
(i.e., applicable to any two terms) way to define such a function. Thus, in this
example, we can see that terms and types are treated differently and are only
related in terms of {\em inhabitance}. This relation is the reason for the
limited expressivity of the STLC and is also the method in which the STLC can be
extended into a more expressive system.

\subsection*{Second-Order (\(\lambda2\))}
\subsubsection*{Role: Addition of polymorphic functions}

Another name for \(\lambda2\) is System F \cite{}. For our purposes, we treat
\(\lambda2\) as an extension of the STLC. This system is adds {\em polymorphic}
terms to the STLC (Dependency (2)). For example, let's take the definition of
{\tt const} above and change it to define {\tt polyConst}:
\begin{center}
\begin{minipage}{0.5\textwidth}
\begin{code}
polyConst : (A B : Set) ‚Üí A ‚Üí B ‚Üí A
polyConst A B = Œª a b ‚Üí a
\end{code}
\end{minipage}
\end{center}
Unlike {\tt const}, {\tt polyConst} takes four arguments, where the first two
are types. These two types (viz. {\tt A} and {\tt B}) are the corresponding
types of the third and fourth arguments of {\tt polyConst}. To clarify, we can
instantiate {\tt polyConst} with \(\mathbb{N}\) and \(\mathbb{B}\) resulting
in a function of type {\tt ‚Ñï ‚Üí ùîπ ‚Üí ‚Ñï}, giving us:
$${\tt const} = {\tt polyConst(‚Ñï,ùîπ)}.$$
Thus, adding polymorphism to the STLC has the effect of extending the notion
of type and term. With \(\lambda2\), we are now allowed to pass types as arguments
at term level (viz. we pass the actual types \(\mathbb{N}\) and \(\mathbb{B}\)).
It is, however, necessary to constrain this expressivity with a more expressive
type (e.g. {\tt (x : Set) ‚Üí ...}). Types that appear in this fashion are
called \(\Pi\)-types. Extending the grammar for simple types with \(\Pi\)-types
results in the grammar for types ({\tt t}) in \(\lambda2\):

\begin{center}
\begin{minipage}{0.5\textwidth}
\begin{code}
Prim || t ‚Üí t || ‚àÄ (x : Set) ‚Üí t
\end{code}


\end{minipage}
\begin{minipage}{0.65\textwidth}
\begin{small}
In general, the \(\forall\) symbol is used to express \(\Pi\)-types
\end{small}
\end{minipage}
\end{center}

Briefly, \(\Pi\)-types are a form of dependent type that are inhabited by
polymorphic functions. In cases like {\tt polyConst}, \(\Pi\)-types are
necessary to signify the treament of a polymorphic function and to prevent
the appearance of free variables within type expressions. For example,
if we had typed {\tt polyConst} as {\tt A ‚Üí B ‚Üí A}, then {\tt A} and {\tt B} are
free variables in that they, by themselves, have no {\em meaning}. This is why
{\tt polyConst} is first passed two types to instantiate the meanings of {\tt A}
and {\tt B}. In a sense, a \(\Pi\)-type binds values for types just as
\(\lambda\)-abstraction binds values for terms.

\(\Pi\)-types are actually significantly more expressive than the ones in
\(\lambda2\). In \(\lambda2\), they are constrained to be only parameterized
over other types (e.g. {\tt Set} in {\tt ‚àÄ (x : Set) ‚Üí t}). Before we can used
\(\Pi\)-types to their full potential, we must first introduce other forms of
type and term dependency.

So far, we've seen how terms can depend on other terms (STLC) and how terms can
depend on types (term level polymoprhism). In the following system, we introduce
a small, yet important dependency: types depending on types.

\subsection*{Higher-order (\(\lambda\underline{\omega}\))}
\subsubsection*{Role: Addition of type constructors}

To clarify, \(\lambda\underline{\omega}\) is generally not a standalone system,
instead, it is generally treated as an ``add-on'' system. Later, we model the
relationship of each system to \(\lambda C\). For the sake of time, we treat
\(\lambda\underline{\omega}\) as an extension of \(\lambda2\), giving us the
following transition:

\begin{center}
\begin{minipage}{0.4\textwidth}
STLC \(\rightarrow\) \(\lambda2\) \(\rightarrow\) (+\(\lambda\underline{\omega}\)) \(\rightarrow\) \(\lambda\omega\)
\end{minipage}
\end{center}

Thus, the system we are introducing is \(\lambda\omega\), which, unlike
\(\lambda\underline{\omega}\), features a more diverse grammar of types (i.e.,
\(\lambda\underline{\omega}\) does not have \(\Pi\)-types). Given its \(\lambda2\)
component, another name for this system is System F\(\omega\). Before we introduce
the new grammar for types, we first introduce the concept of {\em levels}.

We are already familiar with the first two levels, terms and types, which, from
the STLC, are related in terms of inhabitance. To introduce the third level and
onward, we must also provide the type of types. For our purposes, we assume an
infinite universe of types, as is common practice in deriving type theoretic
systems to avoid certain paradoxes \cite{}. For example, let's first assume that
natural numbers are primitive within our system. We then have the following for
the natural number 42:

\begin{center}
\begin{minipage}{0.4\textwidth}
\begin{code}
42 : ‚Ñï : Set : Set‚ÇÅ ...
\end{code}
\end{minipage}
\end{center}

We read the above example as: 42 is of type \(\mathbb{N}\) which is of type
{\tt Set} which of type {\tt Set‚ÇÅ} and so on. There is, however, an important
distinction for the first two levels: 42 is NOT of type {\tt Set}, while
\(\mathbb{N}\) is both of type {\tt Set}, {\tt Set‚ÇÅ} and onwards. Terms are
therefore treated uniquely, since they only exist as elements of types.

We can now revise the definition of types ({\tt t}) to include all instances of
{\tt Set} in our infinite universe. We notate this as {\tt Set} below:

\begin{center}
\begin{minipage}{0.65\textwidth}
\begin{code}
Prim || t ‚Üí t || ‚àÄ (x : Set) ‚Üí t || Set
\end{code}
\end{minipage}
\end{center}
The curious effect of doing this allows us to construct expressions of type
\mbox{{\tt t ‚Üí t}}, where either {\tt t} can have subexpressions containing {\tt Set}.
Let's take the familiar definition of the list data structure:

\begin{center}
\begin{minipage}{0.65\textwidth}
\begin{code}
data List (A : Set) : Set where
  Nil  : List A
  _::_ : A ‚Üí List A ‚Üí List A
\end{code}
\end{minipage}
\end{center}

We discuss {\tt List} more in depth in a later section. For now, we simply ask
{\em what is the type of {\tt List}?} To answer this question, we can simply
look at the first line of the definition above (viz. {\tt List (A : Set) : Set}).
From this line, we can see that {\tt List} is a type parameterized over any type
{\tt A}. Another way to think about this statement is to think of {\tt List} as
a form of polymorphic function of type {\tt Set ‚Üí Set}, as is similar with other
{\em parameterized types}. Unlike like {\tt const} or {\tt polyConst}, this
function can only be applied at type level (thus the name {\em type constructor}).
This is to insure that we cannot write type expressions that fail to instantiate
{\tt List}. For example, the type expression {\tt List ‚Üí A} is incorrect since
{\tt List} cannot appear in a type expression without first being instantiated.
To correct this, we can write this type expression as {\tt List A ‚Üí A}.

From this we can see that the addition of {\tt Set} into our grammar of
types has great effect on the expressivity of our system. There is only one
other extension necessary to consider our system fully-dependently typed, which,
similar to our treatment of \(\lambda\underline{\omega}\), is also deceptively
simple.

\subsection*{Predicates (\(\lambda P\))}
\subsubsection*{Role: Encoding of Propositional logic}

To review, if we compile all the previously mentioned systems above, we have
the system \(\lambda\omega\) (System F\(\omega\)). Our final addition, the
system of \(\lambda P\), allows us to construct {\em type families} which are
themselves types and are meant to directly correspond to propositions in
higher-order logic.

Let's take a simple example. Let's define a proposition {\tt isEven} that takes
a natural number. Another way to say this is that {\tt isEven} is a predicate
over natural numbers. When writing propositions, we write them as their equivalent
type definition (similar to {\tt List}). Consequently, the proposition
{\tt isEven} is an example of a dependent type, since its {\em meaning} depends
on the natural number passed to it (types depending on terms).

\begin{center}
\begin{minipage}{0.7\textwidth}
\begin{code}
data _isEven : ‚Ñï ‚Üí Set where
  ZEven : 0 isEven
  NEven : ‚àÄ n ‚Üí n isEven ‚Üí (Suc (Suc n)) isEven
\end{code}
\end{minipage}
\end{center}
Here, we've defined two ways in which {\tt n isEven} for all natural numbers,
{\tt n}, can be proven. Returning to the idea of proof-relevant mathematics,
we do this by use of its two type constructors {\tt ZEven} and {\tt NEven},
corresponding to the two constructors of \(\mathbb{N}\). {\tt ZEven} is the
proof that 0 is an even number. {\tt NEven} then defines the property that if
a given natural number, {\tt n}, is even (viz. {\tt n isEven}), then {\tt n + 2}
is even (viz. {\tt (Suc (Suc n)) isEven}). To clarify how this type can be used,
let's prove that 4 is even. We start by providing the appropriate type definition
and also providing a name for our proof (viz. {\tt 4isEven}):
\begin{center}
\begin{minipage}{0.5\textwidth}
\begin{code}
4isEven : 4 isEven
\end{code}
\end{minipage}
\end{center}
From the definition of {\tt isEven}, we know that to prove 4 is even, we must
know that 2 is even. Consequently, to prove 2 is even, we must know that 0 is
even. Fortunately, we have a proof that 0 is even: {\tt ZEven}. Thus, the proof
that 2 is even is {\tt NEven 0 ZEven}, and finally the proof that 4 is even is:
\begin{center}
\begin{minipage}{0.5\textwidth}
\begin{code}
4isEven : 4 isEven
4isEven = NEven 2 (NEven 0 ZEven)
\end{code}
\end{minipage}
\end{center}

This method of proof is adopted from {\em Intuitionistic Logic} \cite{}. In
this form of logic, ``truth'' corresponds to the idea of {\em type inhabitability}.
This makes sense since we use types to correspond to logical propositions
(e.g. {\tt isEven}). ``Falsity'' then naturally corresponds to the un-inhabitability
of a given type. This can be seen if we try to prove that 5 is even using
{\tt isEven}. The proof seems to get ``stuck'' at trying to prove that 1 is even.

While it might seem intuitive and more effective to instead try to prove that 5
is {\em not} even, this, however, also does not work. This is due to the
treatment of negation in Intuitionistic Logic. To prove that 5 is not even is
to prove that \(\neg (5\) {\tt isEven}), which is equivalent to constructing a
function of type:
\begin{center}
\begin{minipage}{0.3\textwidth}
\begin{code}
5 isEven ‚Üí ‚ä•
\end{code}
\end{minipage}
\end{center}
That is, given a proof that {\tt 5 isEven}, construct \(\perp\), the empty type.
This is not possible since we cannot ever construct a proof that 5 is even. We
discuss the empty type later in Section 3.

\subsection*{Calculus of Constructions and Remarks}
To conclude our discussion of dependent types, we present the final grammar for
types after extending \(\lambda\omega\) with \(\lambda P\), resulting in
\(\lambda C\):
\begin{center}
\begin{minipage}{0.6\textwidth}
\begin{code}
Prim || t ‚Üí t || ‚àÄ (x : t) ‚Üí t || Set
\end{code}
\end{minipage}
\end{center}
It should come with no surprise that the change is minor. We simply have lifted
the restriction on \(\Pi\)-types and allowed them to be parameterized over both
terms and types.

In the process of introducing dependent types, we have shown the necessary
changes and extensions on the STLC to achieve the expressivity necessary to
function as a foundation for mathematics. To clarify this process and to show
the relationship between each of the systems we have previously mentioned, we
include a diagram of the {\em Lambda Cube} \cite{}:
\begin{center}
  \begin{tikzpicture}[
      back line/.style={densely dotted},
      cross line/.style={preaction={draw=white, -,
          line width=6pt}}]
    \matrix (m) [matrix of math nodes,
      row sep=1em, column sep=.75em,
      text height=1ex,
      text depth=0.5ex]{
      & \lambda\omega & & \lambda C \\
      \lambda 2 & & \lambda P2 \\
      & \lambda\underline\omega & & \lambda P\underline\omega \\
      \lambda\!\!\rightarrow  & & \lambda P \\
    };
    \path[-]
    (m-1-2) edge (m-1-4)
    edge (m-2-1)
    edge [back line] (m-3-2)
    (m-1-4) edge (m-3-4)
    edge (m-2-3)
    (m-2-1) edge [cross line] (m-2-3)
    edge (m-4-1)
    (m-3-2) edge [back line] (m-3-4)
    edge [back line] (m-4-1)
    (m-4-1) edge (m-4-3)
    (m-3-4) edge (m-4-3)
    (m-2-3) edge [cross line] (m-4-3);
  \end{tikzpicture}
\end{center}
Reading the cube requires us to start on the edge marked \(\lambda\!\!\rightarrow\),
which corresponds to the STLC, the base system. From here, we can either move
inwards, upwards, or rightwards, which corresponds to the extension to the other
systems marked on the respective edges. Using these three movements, we
eventually reach \(\lambda C\), leaving it in the most powerful position
on the cube.

\section{Martin-L\"of Type Theory}
%% Curry-Howard?
In this section, we introduce the fundamental concepts of Martin-L\"of Type
Theory \cite{}. We discuss recursion, induction, and as well several primitive type
encodings (from propositional logic).

\subsection*{Recursion and Induction Principles}
Recursion and induction are one of the powerful tools available in a
fully-dependently typed system. Recursion and induction principles can be
thought of as specialized functions to behave on elements of a given type.
If we use recursion and induction principles to define functions and proofs,
we are guaranteed both termination and totality. This is because, in general,
recursion and induction principles are defined under certain known intuitions
(which is why they are sometimes considered axioms) on the given type that
they are meant to act on.

These principles follow a certain pattern:
\begin{center}
\begin{minipage}{0.9\textwidth}
\begin{code}
recT : (C : Set) ‚Üí C‚ÇÄ ... ‚Üí T ‚Üí C
indT : (C : T ‚Üí Set) ‚Üí (C‚ÇÄ : ...) ... ‚Üí (t : T) ‚Üí C t
\end{code}
\end{minipage}

\begin{minipage}{0.7\textwidth}
\begin{small}
The recursion and induction principles for some type {\tt T}.
\end{small}
\end{minipage}
\end{center}
The first argument given to these principles is sometimes called the {\em motive}.
This simply means that {\tt C} is the type from which we will return an element
from. For recursion principles, this means the given output of the function we
are defining, and for induction principles, this can be read as the proposition
we are trying to prove. The terms that follow (viz. {\tt C‚ÇÄ}) correspond to the
constructor(s) of the given type, which is how these principles guarantee
totality. Finally, both principles take a term of the type that are acting on
(viz. {\tt T} and {\tt (t : T)}). The only difference is that the induction
principle introduces a type family (e.g. similar to {\tt isEven}), while a
recursion principle does not. 

In this section, we discuss the recursion and induction principles, {\tt rec}
and {\tt ind}, for each {\em primitive type}: \(\perp\), \(\top\), +,
and \(\times\). The symbols + and \(\times\) signify disjunction and conjunction
and, thus, should not be confused with addition and multiplication. For the
latter, we use the names {\tt plus} and {\tt times}.

\subsection*{Primitive Type Encodings}
%% Bottom, Top, Disjunction, Conjunction, Sigma
We start with \(\perp\), the empty type. Since there no ways to construct
an element of the empty type, the encoding is rather simple:
\begin{center}
\begin{minipage}{0.9\textwidth}
\begin{code}
data ‚ä• : Set where

rec‚ä• : (C : Set) ‚Üí ‚ä• ‚Üí C
rec‚ä• C ()

ind‚ä• : (C : ‚ä• ‚Üí Set) ‚Üí (z : ‚ä•) ‚Üí C z
ind‚ä• C ()

prop : {A : Set} ‚Üí ‚ä• ‚Üí A
prop {A} perp = rec‚ä• A perp

prop' : {A : ‚ä• ‚Üí Set} ‚Üí (x : ‚ä•) ‚Üí A x
prop' {A} perp = ind‚ä• A perp

absurd : ‚ä• ‚Üí (1 ‚â° 2)
absurd = rec‚ä• (1 ‚â° 2) 
\end{code}
\end{minipage}
\end{center}
The behavior of the empty type is meant to model {\it ex-falso} (i.e., from
absurdity comes anything we like). In general, we use the empty type when
writing proofs containing negated expressions.

To clarify our discussion about the empty type, we include a proof of:
$$\neg\neg\neg A\rightarrow\neg A$$
Here, we define negation, \(\neg\), to be a function, such that:
\begin{center}
\begin{minipage}{0.9\textwidth}
\begin{code}
¬¨ : Set ‚Üí Set
¬¨ A = A ‚Üí ‚ä•

prop‚ÇÄ : {A : Set} ‚Üí ¬¨ (¬¨ (¬¨ A)) ‚Üí ¬¨ A
prop‚ÇÄ ¬¨¬¨¬¨a a = ?
\end{code}
\end{minipage}
\end{center}
Now, we solve for {\tt ?}. Looking at the type expression of {\tt prop‚ÇÄ},
we know we have an expression of type \(\neg\neg\neg A\) and also an
expression of type \(A\). To clarify, we can rewrite the type expression
after desugaring \(\neg A\):
$$\neg\neg\neg A\rightarrow A\rightarrow\perp$$
Now, to construct an element of type \(\neg A\) from an element of type
\(\neg\neg\neg A\), we can use the definition of \(\neg\):
$$(\neg\neg A\rightarrow\perp)\rightarrow A\rightarrow\perp$$
$$((\neg A\rightarrow\perp)\rightarrow\perp)\rightarrow A\rightarrow\perp$$
$$(((A\rightarrow\perp)\rightarrow\perp)\rightarrow\perp)\rightarrow A\rightarrow\perp$$
Thus, we know we must apply our expression \(\neg\neg\neg A\) to a function
of type \(\neg\neg A\) and so on, such that:
\begin{center}
\begin{minipage}{0.9\textwidth}
\begin{code}
prop‚ÇÄ : {A : Set} ‚Üí ¬¨ (¬¨ (¬¨ A)) ‚Üí ¬¨ A
prop‚ÇÄ ¬¨¬¨¬¨a a = ¬¨¬¨¬¨a (Œª ¬¨a ‚Üí ¬¨a a)
\end{code}
\end{minipage}
\end{center}
Or equivalently:
\begin{center}
\begin{minipage}{0.9\textwidth}
\begin{code}
prop‚ÇÄ ¬¨¬¨¬¨a a = rec‚ä• ‚ä• (¬¨¬¨¬¨a (Œª ¬¨a ‚Üí ¬¨a a))
\end{code}
\end{minipage}
\end{center}
For clarity, we annotate terms with their respective type (i.e., {\tt ¬¨¬¨¬¨a} is
of type {\tt ¬¨¬¨¬¨A}). We do this quite often when writing proofs.

Next, we introduce the type \(\top\), which contains exactly one element,
{\tt *}:
\begin{center}
\begin{minipage}{0.9\textwidth}
\begin{code}
data ‚ä§ : Set where
  * : ‚ä§

rec‚ä§ : (C : Set) ‚Üí C ‚Üí ‚ä§ ‚Üí C
rec‚ä§ C c * = c

ind‚ä§ : (C : ‚ä§ ‚Üí Set) ‚Üí C * ‚Üí (x : ‚ä§) ‚Üí C x
ind‚ä§ C c * = c
\end{code}
\end{minipage}
\end{center}
Similar to \(\perp\), \(\top\) is simple and is rather uninteresting by itself.
There are, however, some trivial yet interesting proofs that involve the two.
For the first of these, we also introduce a special kind of proof: {\it uniqueness
of identity} (UIP) \cite{}. In this proof, we set out to determine and classify
the identity of every element within a given type. For example, for \(\top\), we
should be able to prove:
$$\forall (x : \top) \rightarrow x ‚â° *$$
That is, for every element of type \(\top\), that element is equivalent to {\tt *}.
This second expression, {\it x ‚â°} {\tt *}, is an example of an {\it identity type},
which we describe later on (Section 4). For now, we simply say that the
sole constructor for the identity type is {\tt (refl x)}, which is a proof that
{\it x ‚â° x}, for any term {\tt x}.

Now, to prove the above proposition, we can use the induction principle for
\(\top\), since the proposition we are trying to prove concerns all elements
of \(\top\):
\begin{center}
\begin{minipage}{0.9\textwidth}
\begin{code}
uip‚ä§ : (x : ‚ä§) ‚Üí x ‚â° *
uip‚ä§ = ind‚ä§ (Œª top ‚Üí top ‚â° *) (refl *)
\end{code}
\end{minipage}
\end{center}
Here, we have taken our motive to be the expression {\tt (Œª top ‚Üí top ‚â° *)},
which essentially repeats the information already given by our type expression
(viz. {\tt (x : ‚ä§) ‚Üí x ‚â° *}). The second element, {\tt (refl *)}, is a proof
that our motive holds in the case that {\tt top} is {\tt *} and is of type
{\tt * ‚â° *}.

Alternatively, we could have written our proof as follows:
\begin{center}
\begin{minipage}{0.9\textwidth}
\begin{code}
uip‚ä§ * = refl *
\end{code}
\end{minipage}
\end{center}
This form of proof uses pattern matching and asserts that the only element in
\(\top\) is {\tt *}, which greatly simplifies our proof. Both methods of proof
are correct, but, in general, using pattern matching is only safe when the
values of proof expressions are not dependent one another (i.e., their values
don't change depending on which expression we apply pattern matching to). This
topic is formally known as {\it dependent pattern-matching}, which is a bit
out of the scope of this paper. Thus, for our purposes, we refrain from using
pattern matching in our proofs and instead use recursion and induction
principles.

Before we can do more proofs, we first introduce the disjunction type, +: 
\begin{center}
\begin{minipage}{0.9\textwidth}
\begin{code}
data _+_ (A B : Set) : Set where
  inl : A ‚Üí A + B
  inr : B ‚Üí A + B

rec+ : {A B : Set} (C : Set) ‚Üí (A ‚Üí C) ‚Üí (B ‚Üí C) ‚Üí
       A + B ‚Üí C
rec+ C f _ (inl a) = f a
rec+ C _ f (inr b) = f b

ind+ : {A B : Set} (C : A + B ‚Üí Set) ‚Üí
       (‚àÄ (x : A) ‚Üí C (inl x)) ‚Üí
       (‚àÄ (x : B) ‚Üí C (inr x)) ‚Üí
       (x : A + B) ‚Üí C x
ind+ C f _ (inl a) = f a
ind+ C _ f (inr b) = f b
\end{code}
\end{minipage}
\end{center}
It might come as surprise as to why the symbol for addition is generally used
for the disjunction type. This is because another name for + is a {\it union type}.
This type is parameterized over two other types, {\tt A} and {\tt B}, and
creates a union between them in that every expression of type {\tt A + B} is
either {\tt (inl a)} or {\tt (inr b)}, where {\tt a} and {\tt b} are typed
{\tt A} and {\tt B}, respectively.

Before we can provide our next example, we must first introduce the
{\em pair type} or {\em logical conjunction}. We also provide the corresponding
recursion and induction principle for the {\tt √ó} type:
\begin{center}
\begin{minipage}{0.9\textwidth}
\begin{code}
data _√ó_ (A B : Set) : Set where
  _,_ : A ‚Üí B ‚Üí A √ó B

rec√ó : {A B : Set} (C : Set) ‚Üí (A ‚Üí B ‚Üí C) ‚Üí A √ó B ‚Üí C
rec√ó C f (a , b) = f a b

ind√ó : {A B : Set} (C : A √ó B ‚Üí Set) ‚Üí
       (‚àÄ (x : A) (y : B) ‚Üí C (x , y)) ‚Üí
       (x : A √ó B) ‚Üí C x
ind√ó C f (a , b) = f a b
\end{code}
\end{minipage}
\end{center}

We can now write a simple proof that shows the union of \(\top\) and \(\top\)
is equivalent to the Boolean type, that is {\tt (‚ä§ + ‚ä§) ‚Üî ùîπ}.
Alternatively, since \(\top\) is occasionally written as 1 and {\tt ùîπ} as 2, we
name our proof as follows:
\begin{center}
\begin{minipage}{0.9\textwidth}
\begin{code}
1+1=2 : (‚ä§ + ‚ä§) ‚Üî ùîπ
1+1=2 = ?
\end{code}
\end{minipage}
\end{center}
To do this, we must associate every possible term of type {\tt ‚ä§ + ‚ä§} to a
given Boolean. This works out quite simply since the type {\tt ‚ä§ + ‚ä§} only has
two possible terms, {\tt (inl *)} and {\tt (inr *)}, which we can associate with
the two terms, {\tt True} and {\tt False}, in the type {\tt ùîπ} and vice-versa.
Thus, we can write the proof as follows:
\begin{center}
\begin{minipage}{0.9\textwidth}
\begin{code}
1+1=2 = <> (f , g)
  where f = rec+ ùîπ (Œª inl* ‚Üí True) (Œª inr* ‚Üí False)
        g = recùîπ (‚ä§ + ‚ä§) (inl *) (inr *)
\end{code}
\end{minipage}
\end{center}
For our purposes, we use the constructor {\tt <>} for constructing a term of
the bijection type. For simplicity, we can define a bijection as a conjunction
of two logical implications:
\begin{center}
\begin{minipage}{0.9\textwidth}
\begin{code}
data _‚Üî_ (A B : Set) : Set where
  <>_ : (A ‚Üí B) √ó (B ‚Üí A) ‚Üí (A ‚Üî B)
\end{code}
\end{minipage}
\end{center}
To clarify, this form of equivalence is called ``judmental equality'' (i.e.,
equality by logical bijection) \cite{}. We go over the three different forms of
equivalence in the following section.

\section{Equivalence and Other Types}
In this section, we present the type of Natural numbers, Lists, and the identity
type. To further understand these types, we also include a brief description of
the different forms of equality, namely definitional, judgmental and
propositional. In type theory, the specific differences between the three are
generally misunderstood but are important in understanding basic type theoretic
proof methods, which we also go into some detail in this section with several
examples.

\subsection*{Equality and Identity Types}
%% talk about definitional equality, judgmental equality, prop equality
The first form of equality is much simpler than the others. This form of
equality is called {\it definitional equality} \cite{}. Simply put, definitional
equality is equality through definition, which requires no proof. For example,
we know that for the natural numbers, {\tt 1 `plus` 1} is equal to {\tt 2} by
the definition of {\tt plus}.

We have already briefly mentioned the second form of equality, judgmental
equality, which is equality by logical bijection. For example, if we have the
propositions {\tt A} and {\tt B}, and from either {\tt A} or {\tt B} we can
derive {\tt B} and {\tt A} respectively, then we can say that {\tt A} and
{\tt B} are judgmentally equivalent. A proof of judgmental equality requires a
proof similar to the one we have presented in the previous section for the
proposition {\tt (‚ä§ + ‚ä§) ‚Üî ùîπ}.

The final form of equivalence is quite a bit more complex than the previous two
and requires a special kind of proof. Propositional equality is equality through
an identity type (e.g. {\tt x ‚â° y}). Thus, we now formally introduce the
definition of the identity type.
%% say something that this is the HoTT version of identity
%% possibly also point out that the discrepancy between the two
%% -- "PROPOSITIONAL EQUALITY"
%% -- For a proposition, C, on a proof of x ‚â° y for
%% -- any x and y, if we can show that it holds for
%% -- (refl x), then C holds for all proofs, p : x ‚â° y.
%% -- "STRICT EQUALITY"
%% -- For a proposition, C, on a reflexive equality (x ‚â° x),
%% -- for any x, if we can show that it holds for (refl x), then
%% -- C holds for all other reflexive equalities p : x ‚â° x.
\begin{center}
\begin{minipage}{0.9\textwidth}
\begin{code}
data _‚â°_ {A : Set} : (a b : A) ‚Üí Set where
  refl : (a : A) ‚Üí (a ‚â° a)
\end{code}
\end{minipage}
\end{center}
Out of all the types we have introduced, the identity type is the first type
parameterized by terms (viz. {\tt a} and {\tt b}). Until now, we have only
introduced types parameterized by other types. Types parameterized by terms are
classified as an {\em indexed types} \cite{}. Essentially, if we think of types
as shapes (i.e., we can think of them as circles containing points representing
their terms), indexed types change their shape depending on which terms are
passed to them. For example, it is not the case that {\tt y ‚â° x} automatically
comes from a proof of {\tt x ‚â° y}. This requires a proof of the symmetry of
identity types.

Before we can prove this, however, we must first introduce the recursion and
induction principles of the identity type:
\begin{center}
\begin{minipage}{0.9\textwidth}
\begin{code}
J : {A : Set} (C : {x y : A} ‚Üí x ‚â° y ‚Üí Set) ‚Üí
    (‚àÄ (x : A) ‚Üí C (refl x)) ‚Üí
    {x y : A} (p : x ‚â° y) ‚Üí C p
J C c (refl x) = c x

rec‚â° : {A : Set} {x y : A} (C : A ‚Üí Set) ‚Üí
       (p : x ‚â° y) ‚Üí C x ‚Üí C y
rec‚â° {A} {x} {y} C = 
  J (Œª {x} {y} _ ‚Üí C x ‚Üí C y)
    (Œª x ‚Üí Œª z ‚Üí z)
    {x} {y} 
\end{code}
\end{minipage}
\end{center}
The more general induction principle for identity types is called {\tt J}.
Later, we discuss a simpler, yet more restricted version. For now, we take
the time to introduce the more general version which is applicable to every
possible identity type \cite{}.

Since identity types are parameterized over two terms, naturally, the motive
parameter (viz. {\tt C}) of their induction principle is also parameterized
by terms (viz. {\tt x} and {\tt y}). The simple reading of {\tt J} is read
similar to other induction principles: given a proposition, {\tt C}, on an
arbitrary identity type, if we can prove that {\tt C} holds for {\tt refl x},
for all {\tt x}, then {\tt C} holds for all identity types. This becomes a bit
more intuitive when we think of what a proof of equality entails: if we want
to prove something for a proof of equality, it suffices to prove it for the
simplest form of equality (i.e., a reflexive equality, {\tt refl}). One can
also note the similarities between {\tt J} and the induction principle for
{\tt ‚ä§}, since these types both have a single constructor (i.e., {\tt refl}
and {\tt *}, respectively).

Now, let's prove the symmetry of identity types using {\tt J}:
\begin{center}
\begin{minipage}{0.9\textwidth}
\begin{code}
sym : {A : Set} {x y : A} ‚Üí x ‚â° y ‚Üí y ‚â° x
sym = J ? ?
\end{code}
\end{minipage}
\end{center}
Next, we must fill in both instances of {\tt ?} in the above proof. Similar to
previous proofs, the first of these is our motive and is simply a restatement of
our type statement. As we have done earlier, we annotate parameterized identity
types with their given equality (viz. {\tt x=y}):
\begin{center}
\begin{minipage}{0.9\textwidth}
\begin{code}
sym : {A : Set} {x y : A} ‚Üí x ‚â° y ‚Üí y ‚â° x
sym = J (Œª {x y} x=y ‚Üí y ‚â° x) ?
\end{code}
\end{minipage}
\end{center}
Now, the last {\tt ?} must be filled in with a proof that our motive holds in
the case that {\tt y} is {\tt x}, that is, we must prove that
{\tt (x : A) ‚Üí x ‚â° x}, which is the type of {\tt refl}. Thus, our completed
proof is the following:
\begin{center}
\begin{minipage}{0.9\textwidth}
\begin{code}
sym : {A : Set} {x y : A} ‚Üí x ‚â° y ‚Üí y ‚â° x
sym = J (Œª {x y} x=y ‚Üí y ‚â° x) refl
\end{code}
\end{minipage}
\end{center}
To reiterate, this is an example of propositional equality. If this were a
definitional or judgmental equality, we would not need to prove symmetry, since
these two forms of equality are automatically reflexive, symmetric, associative,
and transitive. This is the primary and most important difference between other
forms of equality and propositional equality.

A curious fact about {\tt J} is that it does not lend itself to a uniqueness of
identity proof, UIP. In fact, for many reasons, this is actually beneficial and
allows the identity type to be used outside of simple proofs of equality \cite{}.
We discuss this in the final section of this paper and compare {\tt J}
side-by-side with another, less general induction principle for identity types.

\subsection*{Other types}
We now simply provide the remaining types for Natural numbers and lists. With
what we have discussed, these types should not require much explanation.

\subsection*{Natural Numbers}
Natural numbers have two constructors: {\tt 0} and {\tt suc}. The latter of
these is a function that simply adds 1 to any natural number. Natural numbers
are the simplest example of an infinite type (i.e., it encodes every positive
whole number, {\tt n}, such that {\tt n} greater than or equal to {\tt 0}).

\begin{center}
\begin{minipage}{0.9\textwidth}
\begin{code}
data ‚Ñï : Set where
  0 : ‚Ñï
  suc : ‚Ñï ‚Üí ‚Ñï

rec‚Ñï : (C : Set) ‚Üí C ‚Üí (‚Ñï ‚Üí C ‚Üí C) ‚Üí ‚Ñï ‚Üí C
rec‚Ñï C c _ 0 = c
rec‚Ñï C c f (suc n) = f n (rec‚Ñï C c f n) 

ind‚Ñï : (C : ‚Ñï ‚Üí Set) ‚Üí C 0 ‚Üí
       (‚àÄ (x : ‚Ñï) ‚Üí C x ‚Üí C (suc x)) ‚Üí
       (x : ‚Ñï) ‚Üí C x
ind‚Ñï C c _ 0 = c
ind‚Ñï C c f (suc n) = f n (ind‚Ñï C c f n)
\end{code}
\end{minipage}
\end{center}

\subsection*{List/Orderings}
Lists are similar to natural numbers in that their type definition also has two
constructors: {\tt Nil}, which represents the empty list, and a function that
adds an element to the front of a given list, commonly read as {\em cons}.

\begin{center}
\begin{minipage}{0.9\textwidth}
\begin{code}
data List (A : Set) : Set where
  Nil  : List A
  _::_ : A ‚Üí List A ‚Üí List A

recList : {A : Set} ‚Üí (C : Set) ‚Üí C ‚Üí
          (A ‚Üí List A ‚Üí C ‚Üí C) ‚Üí List A ‚Üí C
recList C c f Nil = c
recList C c f (x :: xs) = f x xs (recList C c f xs)

indList : {A : Set} ‚Üí (C : List A ‚Üí Set) ‚Üí C Nil ‚Üí
          (‚àÄ (x : A) ‚Üí (xs : List A) ‚Üí C xs ‚Üí C (x :: xs)) ‚Üí
          (xs : List A) ‚Üí C xs
indList C c f Nil = c
indList C c f (x :: xs) = f x xs (indList C c f xs)
\end{code}
\end{minipage}
\end{center}

\section{Exercises}
We have now seen and discussed all the types necessary to start proving more
complex proofs. As an exercise to the reader, we include a list of propositions
to prove. We invite the reader to use only the appropriate induction and
recursion principles in each proof. It helps to first attempt to prove these
propositions using simple pattern matching. As a hint, use a recursion principle
when the proof behaves like a simple function (i.e., defining {\tt `plus`}) and
induction principle when proving propositions {\tt C} for all terms of a given
type (i.e., like {\tt sym}).

\begin{center}
\begin{minipage}{0.9\textwidth}
\begin{code}
-- Identity Types
congruence : {A : Set} {x y : A} (P : A ‚Üí Set) ‚Üí
             (p : x ‚â° y) ‚Üí P x ‚Üí P y
transitivity : {A : Set} {x y z : A} ‚Üí (x ‚â° y) ‚Üí
               (y ‚â° z) ‚Üí (x ‚â° z)

-- Natural numbers (requires the above)
_plus_ : ‚Ñï ‚Üí ‚Ñï ‚Üí ‚Ñï
plus-left-unit : (i : ‚Ñï) ‚Üí 0 plus i ‚â° i
plus-right-unit : (i : ‚Ñï) ‚Üí i plus 0 ‚â° i
plus-successor : (i j : ‚Ñï) ‚Üí (suc i) plus j ‚â° i plus (suc j)
plus-commutativity : (i j : ‚Ñï) ‚Üí i plus j ‚â° j plus i
plus-associativity : (x y z : ‚Ñï) ‚Üí
                     x plus (y plus z) ‚â° (x plus y) plus z

-- Lists
_++_ : {A : Set} ‚Üí List A ‚Üí List A ‚Üí List A
reverse : {A : Set} ‚Üí List A ‚Üí List A
append-left-unit : {A : Set} (xs : List A) ‚Üí Nil ++ xs ‚â° xs
append-right-unit : {A : Set} (xs : List A) ‚Üí xs ++ Nil ‚â° xs
append-associativity : {A : Set} (xs ys zs : List A) ‚Üí
                       xs ++ (ys ++ zs) ‚â° (xs ++ ys) ++ zs
append-distributivity : {A : Set} (xs ys : List A) ‚Üí
                        reverse (xs ++ ys) ‚â° (reverse ys) ++ (reverse xs) 
double-reverse : {A : Set} (xs : List A) ‚Üí reverse (reverse xs) ‚â° xs

\end{code}
\end{minipage}
\end{center}

\subsection*{Extension to Homotopy Type Theory (maybe)}
\subsection*{Conclusion and Acknowledgments}

\end{document}
